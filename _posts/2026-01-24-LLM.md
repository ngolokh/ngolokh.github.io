---
layout: post
title: "생성형 AI에 관한 기초"
date: 2024-01-24
categories: [ai, Programming]
tags: [AI, chat=gpt, 생성형ai]
toc: true
toc_sticky: true
---

# 생성형 AI에 관한 기초 지식

## 📌생성형 AI란?
**데이터를 단순히 분석,분류하는데 그치지 않고, 학습한 데이터를 바탕으로 '새로운 컨텐츠'를 만들어 내는 인공지능**

| 구분 | 판별형 AI (Traditional AI) | 생성형 AI (Generative AI) |
| :--- | :--- | :--- |
| **목적** | 분류, 예측, 데이터 분석 | 새로운 콘텐츠 및 데이터 생성 |
| **결과물** | 숫자, 카테고리, 확률 (예: 90% 확률로 고양이) | 문장, 이미지, 영상 (예: 고양이 그림) |
| **작동 예시** | 스팸 메일 차단, 넷플릭스 추천 시스템 | 챗봇 대화, 이미지 생성, 소스 코드 작성 |
| **특징** | 정해진 답을 찾는 과정 | 확률적 패턴에 기반한 창의적 결과물 |

## 🌍 인공지능의  흐름

인공지능은 '인간처럼 생각하는 기계'를 만들기 위해 크게 세 번의 패러다임 변화거쳐 지금의 단계에 이르렀다.

### 1. 규칙 기반 AI (Symbolic AI)
초기 AI는 사람이 정의한 논리와 규칙에 따라 움직임.
* **방식:** `if (동물 == '귀가 뾰족함' and '수염이 있음') then '고양이'`
* **한계:** 현실의 복잡성을 모두 담아내지 못하는 '조합의 폭발' 문제에 직면.

### 2. 학습하는 AI (Machine Learning & Deep Learning)
데이터를 통해 스스로 특징을 추출하는 방식.
* **머신러닝:** 사람이 특징(Feature)을 어느 정도 지정해줌.
* **딥러닝:** 인공신경망을 통해 특징 추출까지 AI가 스스로 수행함 (이미지 인식, 번역 등).

### 3. 생성하는 AI (Generative AI)
현재 우리가 경험하고 있는 단계로, 단순히 '무엇인지' 맞추는 것을 넘어 '새로운 것'을 창조.

---
## 🤖 머신러닝이란?

**머신러닝**은 데이터를 분석하고 패턴을 학습하여 미래의 사건을 예측하거나 결정을 내리는 인공지능의 한 분야입니다. 사람이 일일이 규칙을 정해주는 대신, 컴퓨터가 스스로 데이터 속의 수식($$y = f(x)$$)을 찾아내도록 만드는 과정.

---

##  머신러닝의 핵심 분류

### 지도 학습 (Supervised Learning)
입력값과 함께 '정답(Label)'을 제공.
* **분류(Classification):** 이 메일이 스팸인가 아닌가? (Yes/No)
* **회귀(Regression):** 이 아파트의 내년 가격은 얼마인가? (연속적인 숫자 예측)

### 비지도 학습 (Unsupervised Learning)
정답 없이 데이터의 특징만을 보고 그룹을 나눔.
* **군집화(Clustering):** 비슷한 구매 패턴을 가진 고객들끼리 묶기.

### 강화 학습 (Reinforcement Learning)
시행착오를 통해 가장 높은 보상을 받는 행동을 배움.

### 머신러닝 프로젝트의 과정
>성공적인 머신러닝을 위해 모델 자체보단 데이터 전처리가 더 중요

1. 데이터 수집 : 원본 데이터 수집

2. 데이터 전처리 : 결측치 보정, 이상치 제거, 데이터 정규화

3. 모델 선택 및 학습 : 데이터에 적합한  알고리즘 선택 후 학습

4. 평가 및 튜닝 : 테스트 데이터를 통해 검증하고 파라미터 수정


---


## 🧠딥러닝(Deep Learing)
**인간의 뇌 구조를 모방한 인공신경망(ANN)을 여러 층(Deep) 쌓아서 데이터를 학습하는 기술**

# 1. 왜 '딥(Deep)' 일까
입력값과 출력값 사이에 **'은닉층(Hidden Layer)'**이라는 계산 층이 수십, 수백 개씩 존재하기 때문 층이 깊어질수록 AI는 데이터의 단순한 형태에서 시작해 아주 복잡하고 추상적인 특징까지 스스로 파악

# 2. 딥러닝의 학습 원리
1. 순전파 (Forward Propagation): 데이터를 입력하면 각 층을 지나며 가중치($$w$$)와 곱해져 결과값을 도출

2. 손실 계산 (Loss Function): 정답과 AI가 내놓은 답의 차이(오차)를 계산.

3. 역전파 (Backpropagation): 오차를 줄이기 위해 뒤에서부터 앞으로 거꾸로 가며 각 층의 가중치를 미세하게 조정

---
## 머신러닝 🆚 딥러닝

>인공지능이라는 큰 범주 안에 머신러닝이 있으며, 딥러닝은 머신러닝의 특수한 한 갈래

두 기술의 가장 큰 차이는 **데이터 처리 방식**과 **자원 요구량**에 있음.

| 구분 | 머신러닝 (ML) | 딥러닝 (DL) |
| :--- | :--- | :--- |
| **특징 추출** | **수동:** 사람이 직접 데이터의 특징을 정의 | **자동:** AI가 데이터에서 스스로 특징 추출 |
| **데이터 양** | 상대적으로 적은 데이터로도 가능 | **방대한 양의 빅데이터** 필수 |
| **하드웨어** | 저사양 PC (CPU)에서도 실행 가능 | **고성능 GPU** 필수 (병렬 연산) |
| **학습 시간** | 몇 분 ~ 몇 시간 (빠름) | 몇 일 ~ 몇 주 (느림) |
| **해석 가능성** | 결과의 이유를 설명하기 쉬움 | 과정이 복잡한 '블랙박스' 형태 |
---

### 👍핵심 포인트 : "Feature Engineering"

머신러닝과 딥러닝을 가르는 가장 결정적인 기준은 **'특징(Feature)을 누가 추출하느냐'** .

* **머신러닝:** 자동차를 인식시키기 위해 사람이 "바퀴가 있고, 유리창이 있다"는 특징을 직접 코드로 가르쳐줘야 함.
* **딥러닝:** 수만 장의 자동차 사진을 던져주면, 신경망이 픽셀 단위의 패턴을 분석해 자동차의 특징을 **스스로 학습**함.
---

# 자연어 처리(Natural Language Processing, NLP)
>인간의 언어(자연어)를 컴퓨터가 이해하고, 해석하며, 생성할 수 있도록 하는 인공지능의 한 분야

## 🛠️ 자연어 처리의 주요 단계
>자연어 처리는 컴퓨터가 이해할 수 없는 '텍스트'를 '숫자'로 바꾸는 과정에서 시작.

1.**전처리 (Preprocessing)**: 불필요한 공백 제거, 특수문자 제거, 대소문자 통일 등 데이터를 정제.

2.**토큰화 (Tokenization)**: 문장을 단어, 형태소 또는 문자 단위(Token)로 쪼갬.

3.**임베딩 (Embedding)**: 쪼개진 단어들을 고차원 공간상의 숫자로 변환합니다. 이때 비슷한 의미를 가진 단어들은 공간상에서 가까운 위치에 있게 됨.

4.**모델링 (Modeling)**: RNN, LSTM을 거쳐 현재는 문맥 전체를 한 번에 파악하는 트랜스포머(Transformer) 구조가 표준이 되었음.

---
# LLM(Large Language Model, 거대 언어 모델)
>현대 생성형 AI의 '뇌' 역할을 하는 가장 핵심적인 기술입니다. 자연어 처리(NLP) 기술이 비약적으로 발전하여 탄생한 결과물

LLM은 수조 개의 단어로 구성된 방대한 데이터를 학습하여 인간처럼 텍스트를 이해하고 생성할 수 있도록 설계된 인공지능 모델

- Large (거대함): 파라미터(매개변수)의 개수가 수천억 개에 달하며, 학습 데이터 역시 인터넷상의 거의 모든 텍스트를 포함할 정도로 거대

- Language Model (언어 모델): 다음에 올 단어를 확률적으로 예측하여 문장을 완성하는 모델

### 🚚LLM을 가능하게 한 핵심 기술: Transformer

>LLM의 성능이 급격히 좋아진 이유는 2017년 구글이 발표한 **트랜스포머(Transformer)** 구조 덕분.

* **Attention 메커니즘:** 문장 내의 단어들 사이의 관계를 파악하여, 어떤 단어가 중요한지 '집중(Attention)'해서봄.
* **병렬 처리:** 문장을 순차적으로 읽지 않고 한꺼번에 처리하여 학습 속도와 성능을 획기적으로 높임.

### LLM의 주요 특징
| 특징 | 설명 |
| :--- | :--- |
| **창발적 능력 (Emergent Abilities)** | 모델이 커짐에 따라 가르치지 않은 복잡한 추론 능력이 갑자기 나타나는 현상 |
| **Few-shot Learning** | 몇 개의 예시만 주어져도 새로운 작업을 빠르게 습득함 |
| **할루시네이션 (Hallucination)** | 매우 그럴듯하지만 사실 관계가 틀린 답변을 내놓는 현상 (주의 필요) |

---

💪💪💪💪💪